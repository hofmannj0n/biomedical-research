{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmUiF0ZMKsgC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from multiprocessing import Pool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wmm1t_1SNhnO"
      },
      "outputs": [],
      "source": [
        "# Read the medications CSV file and locate the problematic line\n",
        "medications_file = \"medications.csv\"\n",
        "line_number = 92156\n",
        "\n",
        "with open(medications_file, \"r\") as file:\n",
        "    for i, line in enumerate(file, start=1):\n",
        "        if i == line_number:\n",
        "            print(f\"Problematic line {line_number}: {line}\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMFH5gMkK1LE"
      },
      "outputs": [],
      "source": [
        "# Define a function to merge datasets in smaller chunks\n",
        "def merge_datasets(df1, df2, on_column, how, chunk_size):\n",
        "    result = pd.DataFrame()\n",
        "    for chunk_df2 in df2:\n",
        "        if isinstance(chunk_df2, pd.DataFrame):\n",
        "            merged_chunk = pd.merge(df1, chunk_df2, on=on_column, how=how)\n",
        "            result = pd.concat([result, merged_chunk], ignore_index=True)\n",
        "        else:\n",
        "            print(\"Skipping invalid chunk:\", chunk_df2)\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYR9De2hK2a0"
      },
      "outputs": [],
      "source": [
        "# Read the CSV files in smaller chunks\n",
        "diagnosis_chunks = pd.read_csv(\"diagnosis.csv\", chunksize=1000)\n",
        "operations_chunks = pd.read_csv(\"operations.csv\", chunksize=1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuXldlA_K3v5"
      },
      "outputs": [],
      "source": [
        "# Merge diagnosis and operations on subject_id\n",
        "merged_df = pd.DataFrame()\n",
        "for diagnosis_chunk, operations_chunk in zip(diagnosis_chunks, operations_chunks):\n",
        "    merged_chunk = pd.merge(diagnosis_chunk, operations_chunk, on=\"subject_id\", how=\"outer\")\n",
        "    merged_df = pd.concat([merged_df, merged_chunk], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzPdf4N_K5G8"
      },
      "outputs": [],
      "source": [
        "# Read the medications CSV file in chunks\n",
        "medications_chunks = pd.read_csv(\"medications.csv\", chunksize=1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-0FjoOtK6Of"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty DataFrame to store the merged data\n",
        "merged_df_final = pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3dWLmkHK7hV"
      },
      "outputs": [],
      "source": [
        "# Define a function to merge medications chunks with the existing merged data\n",
        "def merge_medications_chunk(chunk_df):\n",
        "    try:\n",
        "        # Check if the DataFrame contains expected number of columns\n",
        "        if len(chunk_df.columns) == 4:\n",
        "            merged_chunk = merge_datasets(merged_df, [chunk_df], \"subject_id\", \"outer\", 1000)\n",
        "            return merged_chunk\n",
        "        else:\n",
        "            print(\"Skipping chunk due to unexpected number of fields.\")\n",
        "            return pd.DataFrame()  # Return an empty DataFrame if unexpected number of fields\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing medications chunk: {e}\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if there's an error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5fwizVqK9oj",
        "outputId": "7d4e673f-2f60-4550-9776-5d37dbdf7b45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing medications chunk 0\n",
            "Processing medications chunk 1\n",
            "Processing medications chunk 2\n",
            "Processing medications chunk 3\n",
            "Processing medications chunk 4\n",
            "Processing medications chunk 5\n",
            "Processing medications chunk 6\n",
            "Processing medications chunk 7\n",
            "Processing medications chunk 8\n",
            "Processing medications chunk 9\n",
            "Processing medications chunk 10\n",
            "Processing medications chunk 11\n",
            "Processing medications chunk 12\n",
            "Processing medications chunk 13\n",
            "Processing medications chunk 14\n",
            "Processing medications chunk 15\n",
            "Processing medications chunk 16\n",
            "Processing medications chunk 17\n",
            "Processing medications chunk 18\n",
            "Processing medications chunk 19\n",
            "Processing medications chunk 20\n",
            "Processing medications chunk 21\n",
            "Processing medications chunk 22\n",
            "Processing medications chunk 23\n",
            "Processing medications chunk 24\n",
            "Processing medications chunk 25\n",
            "Processing medications chunk 26\n",
            "Processing medications chunk 27\n",
            "Processing medications chunk 28\n",
            "Processing medications chunk 29\n",
            "Processing medications chunk 30\n",
            "Processing medications chunk 31\n",
            "Processing medications chunk 32\n",
            "Processing medications chunk 33\n",
            "Processing medications chunk 34\n",
            "Processing medications chunk 35\n",
            "Processing medications chunk 36\n",
            "Processing medications chunk 37\n",
            "Processing medications chunk 38\n",
            "Processing medications chunk 39\n",
            "Processing medications chunk 40\n",
            "Processing medications chunk 41\n",
            "Processing medications chunk 42\n",
            "Processing medications chunk 43\n",
            "Processing medications chunk 44\n",
            "Processing medications chunk 45\n",
            "Processing medications chunk 46\n",
            "Processing medications chunk 47\n",
            "Processing medications chunk 48\n",
            "Processing medications chunk 49\n",
            "Processing medications chunk 50\n",
            "Processing medications chunk 51\n",
            "Processing medications chunk 52\n",
            "Processing medications chunk 53\n",
            "Processing medications chunk 54\n",
            "Processing medications chunk 55\n",
            "Processing medications chunk 56\n",
            "Processing medications chunk 57\n",
            "Processing medications chunk 58\n",
            "Processing medications chunk 59\n",
            "Processing medications chunk 60\n",
            "Processing medications chunk 61\n",
            "Processing medications chunk 62\n",
            "Processing medications chunk 63\n",
            "Processing medications chunk 64\n",
            "Processing medications chunk 65\n",
            "Processing medications chunk 66\n",
            "Processing medications chunk 67\n",
            "Processing medications chunk 68\n",
            "Processing medications chunk 69\n",
            "Processing medications chunk 70\n",
            "Processing medications chunk 71\n",
            "Processing medications chunk 72\n",
            "Processing medications chunk 73\n",
            "Processing medications chunk 74\n",
            "Processing medications chunk 75\n",
            "Processing medications chunk 76\n",
            "Processing medications chunk 77\n",
            "Processing medications chunk 78\n",
            "Processing medications chunk 79\n",
            "Processing medications chunk 80\n",
            "Processing medications chunk 81\n",
            "Processing medications chunk 82\n",
            "Processing medications chunk 83\n",
            "Processing medications chunk 84\n",
            "Processing medications chunk 85\n",
            "Processing medications chunk 86\n",
            "Processing medications chunk 87\n",
            "Processing medications chunk 88\n",
            "Processing medications chunk 89\n",
            "Processing medications chunk 90\n",
            "Processing medications chunk 91\n"
          ]
        },
        {
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Expected 4 fields in line 92156, saw 6\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-596f3ba7a1fe>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Merge medications chunks using multiprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmedications_chunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmedications_chunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processing medications chunk {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmerged_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_medications_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmedications_chunk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1624\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1625\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mget_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1731\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_currow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1733\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1702\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 4 fields in line 92156, saw 6\n"
          ]
        }
      ],
      "source": [
        "# Merge medications chunks using multiprocessing\n",
        "with Pool() as pool:\n",
        "    for i, medications_chunk in enumerate(medications_chunks):\n",
        "        print(f\"Processing medications chunk {i}\")\n",
        "        merged_chunks = pool.map(merge_medications_chunk, [medications_chunk])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xIhEvylK_CW"
      },
      "outputs": [],
      "source": [
        "# Concatenate merged chunks into a single DataFrame\n",
        "for i, chunk in enumerate(merged_chunks):\n",
        "    print(f\"Merging medications chunk {i+1}\")\n",
        "    merged_df_final = pd.concat([merged_df_final, chunk], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSUwjgJ2LAq5"
      },
      "outputs": [],
      "source": [
        "# Select desired columns\n",
        "selected_columns = [\"subject_id\", \"chart_time_x\", \"icd10_cm\", \"op_id\", \"hadm_id\", \"case_id\", \"opdate\", \"sex\", \"weight\", \"height\", \"race\", \"asa\", \"emop\", \"department\", \"antype\", \"ics10_pcs\", \"orin_time\", \"orout_time\", \"opstart_time\", \"opend_time\", \"admission_time\", \"discharge_time\", \"anstart_time\", \"cpbon_time\", \"cpboff_time\", \"icuin_time\", \"icuout_time\", \"inhosp_death_time\", \"drug_name\", \"route\"]\n",
        "merged_df_final = merged_df_final[selected_columns]\n",
        "\n",
        "# Remove duplicate rows based on subject_id\n",
        "merged_df_final = merged_df_final.drop_duplicates(subset=\"subject_id\")\n",
        "\n",
        "# Write to a new CSV file\n",
        "merged_df_final.to_csv(\"merged_data.csv\", index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}